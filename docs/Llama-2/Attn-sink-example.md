<b>Prompt (from Lu et al, arXiv:2309.01809):<\b> “Large language models have exhibited emergent abilities, demonstrating
exceptional performance across diverse tasks for which they were not explicitly trained, including those that require
complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of
research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that
the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting
techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In
this study, we provide the first comprehensive examination of these emergent abilities while accounting for various
potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models,
encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks.”  

<img src="https://github.com/landrewwray/landrewwray.github.io/blob/main/docs/assets/img/Attn_sink_example.png" alt="SMBC Sept. 19 2023" width="500"/>
<br><b>Another example of the attention sink effect versus output token.</b>
