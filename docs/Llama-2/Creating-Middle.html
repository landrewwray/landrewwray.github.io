<html>
<head>
<style>
  body {
    max-width: 800px;
  }
  p.mat {
    max-width: 330px;
  }
</style>
</head>
<body>
    
<h3>“Quick and dirty” recipe for the middle dictionary:</h3>

<ol>
<li>Select a list of words for the dictionary.  I chose the “I like the red ball…” prompt and an additional 500 words randomly selected from this list of 3000 common English words.</li>
<li>Tokenize the words, and enter each one in a 2-token prompt: [&lts&gt, I], [&lts&gt, like], [&lts&gt, the], etc.</li>
<li>Run the model for each 2-token prompt, and save a vector v representing the output of the second layer in the 2nd token location. For each model run:</li>
    <ol>
    <li> A. Remove the dummy (&lts&gt) token component from v, as |v’> = |v&gt - |dummy&gt&ltdummy|v&gt, where &ltdummy|v&gt denotes an inner product, and the v and s vectors are L2 normalized beforehand.  Note that &ltdummy|v’&gt = 0.  </li>
    <li> B. Repeat the procedure in step 3.i. to remove the input dictionary vector component of the 2nd prompt token from v’.  </li>
    <li> C. Normalize the vector and enter it in the new dictionary!  </li>
    </ol>
</ol>

<p>One should bear in mind that this is just a quick stab at reconstructing a "middle dictionary", and it should be easy to do much better. Weaknesses of this approach include:</p>
<ol>
    <li>It probably yields a mixture of content from several dictionaries that are in use in the 2nd layer, including the output dictionary and whatever set of "middle dictionaries" the model creates.</li>
    <li>The vocabulary of 'meanings' encoded by the middle dictionary does not need to be identical to the input dictionary.  By forcing a mapping onto tokens, it's likely that we're missing elements of relational context or other syntax that may exist within the middle dictionary.</li>
</ol>
    
<p></p>

</body>
</html>
